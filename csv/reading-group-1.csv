"date","time","speaker","url","affiliation","title","abstract","bio","imgfile","location","zoom","connected-papers"
"08/02/22","12:00","Dimitris Tsipras","https://dtsipras.com/","Stanford","Thinking about the worst (in machine learning)","While modern machine learning systems can succeed on challenging benchmarks, they are quite brittle: their performance significantly degrades when exposed to even small variations of their training environments.
How can we build ML models that are more robust? In this talk, I will present a framework for reasoning about the worst-case behavior of our machine learning systems. I will then describe how this framework can be used in practice to train models that are invariant to a broad family of worst-case input perturbations. Finally, I will discuss how such robust learning can be fundamentally different from standard learning, presenting a unique set of challenges and opportunities.","Dimitris Tsipras is a postdoctoral researcher at Stanford, advised by Percy Liang and Greg Valiant. He obtained his PhD from MIT, where he was advised by Aleksander Madry. His work ocuses on understanding and improving the reliability of modern machine learning methods.","tsipras.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2","Towards Deep Learning Models Resistant to Adversarial Attacks;Robustness May Be at Odds with Accuracy"
"08/02/22","11:00","Marc Lanctot","http://mlanctot.info/","DeepMind","Mastering the game of Go with deep neural networks and tree search","The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. In this talk I will cover AlphaGo, an approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0, and later to defeat a previous Go professional Lee Sedol. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go. Toward the end of the talk, I will give a short survey of what has happened since this landmark event in AI and some open challenges for continued work in this area.","Marc Lanctot is a research scientist at DeepMind. Previously, he was a post-doctoral researcher at the Maastricht University Games and AI Group, working with Mark Winands. During his PhD, he worked at University of Alberta with Michael Bowling on sampling algorithms for equilibrium computation and decision-making in games. Before his PhD, he did an undergrad and Master's at McGill University's School of Computer Science and Games Research @ McGill, under the supervision of Clark Verbrugge. He is interested in general multiagent learning (and planning), computational game theory, reinforcement learning, and game-tree search.","marc_jun_2021.png","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2","A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play."
"15/02/22","11:00:00","Binghui Peng","http://www.cs.columbia.edu/~binghuip/","Columbia","Efficient Regret Minimization in Non-Convex Games","We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.","Binghui Peng is a third-year PhD student (Fall 2019 - present) in the computer science department of Columbia University, advised by Prof. Christos Papadimitriou and Prof. Xi Chen. He is broadly interested in theoretical computer science, especially its intersection with machine learning and game theory. In summer 2020, he worked in Google research NYC, hosted by Sara Ahmadian. Previously, he got his bachelor degree from Yao class, Tsinghua Univeristy, where he worked with Prof. Pingzhong Tang on game theory.","no_photo.png","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2","Efficient Regret Minimization in Non-Convex Games"
"15/02/22","12:00:00","Kiran Vodrahalli","https://kiranvodrahalli.github.io/about/","Columbia","The Platform Design Problem Abstract","On-line firms deploy suites of software platforms, where each platform is designed to interact with users during a certain activity, such as browsing, chatting, socializing, emailing, driving, etc. The economic and incentive structure of this exchange, as well as its algorithmic nature, have not been explored to our knowledge. We model this interaction as a Stackelberg game between a Designer and one or more Agents. We model an Agent as a Markov chain whose states are activities; we assume that the Agent's utility is a linear function of the steady-state distribution of this chain. The Designer may design a platform for each of these activities/states; if a platform is adopted by the Agent, the transition probabilities of the Markov chain are affected, and so is the objective of the Agent. The Designer's utility is a linear function of the steady state probabilities of the accessible states minus the development cost of the platforms. The underlying optimization problem of the Agent -- how to choose the states for which to adopt the platform -- is an MDP. If this MDP has a simple yet plausible structure (the transition probabilities from one state to another only depend on the target state and the recurrent probability of the current state) the Agent's problem can be solved by a greedy algorithm. The Designer's optimization problem (designing a custom suite for the Agent so as to optimize, through the Agent's optimum reaction, the Designer's revenue), is NP-hard to approximate within any finite ratio; however, the special case, while still NP-hard, has an FPTAS. These results generalize from a single Agent to a distribution of Agents with finite support, as well as to the setting where the Designer must find the best response to the existing strategies of other Designers. We discuss other implications of our results and directions of future research. ","Kiran Vodrahalli is a 5th year Ph.D. student at Columbia University advised by Daniel Hsu and Alex Andoni. He previously received a B.A. in Mathematics and an M.S.E in Computer Science from Princeton University, where he was advised by Sanjeev Arora.","kiran.png","Zoom","https://berkeley.zoom.us/my/simonszoom2",
"01/03/22","11:00:00","Tatjana Chavdarova","https://chavdarova.github.io/","Berkeley"," Continuous Dynamics: Failures & Successes in Saddle Point Problems","There will be two parts, in the first one I will present the work by Hsieh, Mertikopoulos and Cevher from ICML 2021, and in the second part I will present our joint work with Manolis Zampetakis and Michael I. Jordan -- see their abstracts below, respectively.

Compared to minimization, the min-max optimization in machine learning applications is considerably more convoluted because of the existence of cycles and similar phenomena. Such oscillatory behaviors are well-understood in the convex-concave regime, and many algorithms are known to overcome them. In this paper, we go beyond this basic setting and characterize the convergence properties of many popular methods in solving non-convex/non-concave problems. In particular, we show that a wide class of state-of-the-art schemes and heuristics may converge with arbitrarily high probability to attractors that are in no way min-max optimal or even stationary. Our work thus points out a potential pitfall among many existing theoretical frameworks, and we corroborate our theoretical claims by explicitly showcasing spurious attractors in simple two-dimensional problems.

Several widely-used first-order saddle point optimization methods yield an identical continuous-time ordinary differential equation (ODE) to that of the Gradient Descent Ascent (GDA) method when derived naively. However, their convergence properties are very different even on simple bilinear games. We use a technique from fluid dynamics called High-Resolution Differential Equations (HRDEs) to design ODEs of several saddle point optimization methods. For bilinear games, the convergence properties of the derived HRDEs correspond to that of the starting discrete methods. Using these techniques, we show that the HRDE of Optimistic Gradient Descent Ascent (OGDA) has last-iterate convergence for general monotone variational inequalities. To our knowledge, this is the first proof in continuous-time for monotone variational inequalities. Moreover, we provide the rates for the best-iterate convergence of the OGDA method, relying solely on the first-order smoothness of the monotone operator.

","Tatjana Chavdarova is a Postdoctoral Researcher at the Department of Electrical Engineering and Computer Science (EECS) at the University of California at Berkeley, working with Michael Jordan. Prior to that she was postdoc at the Machine Learning and Optimization (MLO) lab at EPFL, working with Martin Jaggi, and she obtained her Ph.D. from EPFL, and Idiap, supervised by François Fleuret. She is primarily interested in the intersection of game theory and machine learning, but also generalization, robustness, and generative modeling.","chavdarova.png","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2","https://arxiv.org/pdf/2112.13826.pdf;https://arxiv.org/pdf/2112.13826.pdf


"
"08/03/22","11:00:00","Jason Milionis","https://www.linkedin.com/in/jasonmili","Columbia","Dynamical Systems, Games, and Topology: a relationship in formation","A basic question that has arisen ever since the genesis of the concept of the Nash Equilibrium [Nash, 1951] is whether it can adequately capture the long-term behavior of players who play a game repeatedly. Considering the case where the players’ behavior is represented by a rule/map from the current mixed strategy to the next, this becomes a problem in the realm of dynamical systems. In this context, we will ponder to what extent we might (or might not) expect Nash Equilibria to be attractors of dynamical systems. At the same time, we will wonder: What does it even mean for a dynamical system to converge, when one might observe chaotic-like behavior? Our journey will include an expansive introduction to some notions of the mathematical field of Algebraic Topology, and we will showcase the power of topological arguments in our effort to understand the limits of Nash Equilibria. The culmination of that discussion will be a general impossibility result: there exist games for which no game dynamics’ long-term behavior can be described by Nash Equilibria in a satisfactory way. A similar, but even stronger, result holds, if we had desired that approximate Nash Equilibria be the entire ""set of fate"" of any game dynamics; there, there is a positive-measure set of games for which the set of approximate Nash Equilibria cannot accurately characterize the fate of the dynamical system. Nonetheless, complexity will also manifest itself in our discussion, since we will show that for non-degenerate games in particular, we can construct such game dynamics, albeit with complexity-wise obstacles this time, and we will end our exploration with our conjecture that constructing such a game dynamics for non-degenerate games is PPAD-complete.

The talk is based on a recently submitted joint work with Christos Papadimitriou (Columbia University), Georgios Piliouras (Singapore University of Technology and Design), and Kelly Spendlove (University of Oxford).","Jason Milionis is a first-year Ph.D. student in the Computer Science Department at Columbia University, advised by Christos Papadimitriou and Tim Roughgarden. He is broadly interested in Game Theory, especially in combination with Machine Learning. Previously, he graduated with a bachelor's degree in Electrical and Computer Engineering with a major in Computer Science from the National Technical University of Athens (NTUA).


","milionis.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2",
"15/03/22","11:00:00","Noah Golowich","https://noahgol.github.io/","MIT","Smoothed online learning is as easy as statistical learning","Much of modern learning theory has been split between two regimes: the classical offline setting, where data arrive independently, and the online setting, where data arrive adversarially. While the former model is often both computationally and statistically tractable, the latter re- quires no distributional assumptions. In an attempt to achieve the best of both worlds, previous work proposed the smooth online setting where each sample is drawn from an adversarially chosen distribution, which is smooth, i.e., it has a bounded density with respect to a fixed dom- inating measure. Existing results for the smooth setting were known only for binary-valued function classes and were computationally expensive in general; in this paper, we fill these lacunae. In particular, we provide tight bounds on the minimax regret of learning a nonpara- metric function class, with nearly optimal dependence on both the horizon and smoothness parameters. Furthermore, we provide the first oracle-efficient, no-regret algorithms in this set- ting. In particular, we propose an oracle-efficient improper algorithm whose regret achieves optimal dependence on the horizon and a proper algorithm requiring only a single oracle call per round whose regret has the optimal horizon dependence in the classification setting and is sublinear in general. Both algorithms have exponentially worse dependence on the smoothness parameter of the adversary than the minimax rate. We then prove a lower bound on the oracle complexity of any proper learning algorithm, which matches the oracle-efficient upper bounds up to a polynomial factor, thus demonstrating the existence of a statistical-computational gap in smooth online learning. Finally, we apply our results to the contextual bandit setting to show that if a function class is learnable in the classical setting, then there is an oracle-efficient, no-regret algorithm for contextual bandits in the case that contexts arrive in a smooth manner.","Noah Golowich is a PhD Student at Massachusetts Institute of Technology, advised by Constantinos Daskalakis. His research interests are in theoretical machine learning, with particular focus on the connections between multi-agent learning, game theory, and online learning. He is a recipient of a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Fellowship.","noah.png","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2",
"15/03/22","12:00:00","Chen-yu Wei","https://bahh723.github.io/","USC","Decentralized Cooperative Reinforcement Learning with Hierarchical Information Structure","Multi-agent reinforcement learning (MARL) problems are challenging due to information asymmetry. To overcome this challenge, existing methods often require high level of coordination or communication between the agents. We consider two-agent multi-armed bandits (MABs) and Markov decision processes (MDPs) with a hierarchical information structure arising in applications, which we exploit to propose simpler and more efficient algorithms that require no coordination or communication. In the structure, in each step the “leader” chooses her action first, and then the “follower” decides his action after observing the leader’s action. The two agents observe the same reward (and the same state transition in the MDP setting) that depends on their joint action. For the bandit setting, we propose a hierarchical bandit algorithm that achieves a near-optimal gap-independent regret of Oe( √ ABT) and a near-optimal gap-dependent regret of O(log(T )), where A and B are the numbers of actions of the leader and the follower, respectively, and T is the number of steps. We further extend to the case of multiple followers and the case with a deep hierarchy, where we both obtain near-optimal regret bounds. For the MDP setting, we obtain Oe( √ H7S2ABT ) regret, where H is the number of steps per episode, S is the number of states, T is the number of episodes. This matches the existing lower bound in terms of A, B, and T .","Chen-Yu Wei is a fifth-year Computer Science PhD student at University of Southern California. He received M.S. in Communication Engineering and B.S. in Electrical Engineering from National Taiwan University. His research focuses on designing provable algorithms for online decision making, reinforcement learning, and learning in games. He is a recipient of the Best Paper Award at COLT 2021, and the Simons-Berkeley Research Fellowship. ","chen-yu-wei.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2",
"22/03/22","11:00:00","Vidya Muthukumar","https://vmuthukumar.ece.gatech.edu/","GA Tech","No-regret learning in repeated games under realization-based feedback: Time-averaged and last-iterate convergence","This talk reviews recent advances in the study of two related objectives for no-regret learning in repeated (will mostly focus on zero-sum) finite normal-form games — fast time-averaged convergence to Nash equilibrium, and last-iterate convergence to Nash equilibrium — from the point of view of “realization-based” feedback. We note a critical distinction between the “telepathic” feedback model, where players observe their opponents’ mixtures, and the “realization-based” feedback model, where players observe only their opponents’ realizations. At a high level, the realization-based model introduces extra stochasticity into the system which makes it harder to achieve either fast rates of time-averaged convergence or last-iterate convergence.

We first briefly review the fast time-averaged convergence results to zero-sum Nash equilibrium provided for optimistic algorithms by Rakhlin and Sridharan [RS13], and explain why these fast rates do not in general continue to hold under realization-based feedback. Next, we introduce the class of smooth games for which [FLLST16] show fast time-averaged convergence rates under realization-based feedback, and elucidate the special properties of smooth games that make this possible. Finally, we introduce our recent work that provides impossibility results for the last-iterate convergence of a large class of optimal no-regret algorithms for general zero-sum games, including optimistic and data-adaptive online algorithms [MPS20]. 

(This talk will primarily use and explain tools from discrete-time adversarial online learning and regret minimization. However, related perspectives from stochastic optimization and dynamical systems theory may be mentioned when relevant.) 

References:
[RS13] Alexander Rakhlin and Karthik Sridharan: “Optimization, learning, and games with predictable sequences”.
[FLLST16] Dylan J. Foster, Zhiyuan Li, Thodoris Lykouris, Karthik Sridharan, Eva Tardos: “Learning in games: Robustness of fast convergence”.
[MPS20] Vidya Muthukumar, Soham Phade and Anant Sahai: “On the impossibility of convergence of mixed strategies arising from optimal no-regret learning”.","Vidya Muthukumar is an Assistant Professor in the Schools of Electrical and Computer Engineering and Industrial and Systems Engineering at Georgia Institute of Technology. Her broad interests are in game theory, online and statistical learning. She is particularly interested in designing learning algorithms that provably adapt in strategic environments, the intersection of algorithmic game theory and reinforcement learning, and the theory of overparameterized machine learning.

Vidya received the PhD degree in Electrical Engineering and Computer Sciences from University of California, Berkeley. She is the recipient of the Adobe Data Science Research Award, the Simons-Berkeley Research Fellowship (for the Fall 2020 program on ""Theory of Reinforcement Learning""), and a Georgia Tech Class of 1969 Teaching Fellowship for the academic year 2021-2022.","vidya.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2",
"22/03/22","12:00:00","Ioannis Panageas","https://panageas.github.io/","UC Irvine","Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games","Potential games are arguably one of the most important and widely studied classes of normal form games. They define the archetypal setting of multi-agent coordination as all agent utilities are perfectly aligned with each other via a common potential function. Can this intuitive framework be transplanted in the setting of Markov Games? What are the similarities and differences between multi-agent coordination with and without state dependence? We present a novel definition of Markov Potential Games (MPG) that generalizes prior attempts at capturing complex stateful multi-agent coordination. Counter-intuitively, insights from normal-form potential games do not carry over as MPGs can consist of settings where state-games can be zero-sum games. In the opposite direction, Markov games where every state-game is a potential game are not necessarily MPGs. Nevertheless, MPGs showcase standard desirable properties such as the existence of deterministic Nash policies. In our main technical result, we prove fast convergence of independent policy gradient (and its stochastic variant) to Nash policies by adapting recent gradient dominance property arguments developed for single agent MDPs to multi-agent learning settings.  ","Ioannis is an Assistant Professor of Computer Science at UCI. He is interested in the theory of computation, machine learning and its interface with non-convex optimization, dynamical systems, multi-agent reinforcement learning, probability and statistics. Before joining UCI, he was an Assistant Professor at Singapore University of Technology and Design. Prior to that he was a MIT postdoctoral fellow. He received his PhD in Algorithms, Combinatorics and Optimization from Georgia Tech in 2016, a Diploma in EECS from National Technical University of Athens, and a M.Sc. in Mathematics from Georgia Tech. He is the recipient of the 2019 NRF fellowship for AI.

","panageas.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2",
"26/04/22","11:00:00","Yu Bai","https://yubai.org/","Salesforce AI Research","Near-Optimal Learning of Extensive-Form Games with Imperfect Information

","Imperfect Information Games such as Poker constitute an important challenge for modern artificial intelligence. In this talk we consider the problem of learning Imperfect-Information Extensive-Form Games (IIEFGs), a celebrated formulation for games involving both imperfect information and sequential play. IIEFGs is a generalization of normal-form games, and is related to (but poses quite different challenges from) Markov Games.In the first part of the talk, we will review the definition and basic properties of IIEFGs, and go through existing algorithms such as Online Mirror Descent and Counterfactual Regret Minimization. In the second part of the talk, we present our new result---A first line of algorithms that require only $\widetilde{\mathcal{O}}((XA+YB)/\varepsilon^2)$ episodes of play to find an $\varepsilon$-approximate Nash equilibrium in two-player zero-sum IIEFGs, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\widetilde{\mathcal{O}}((X^2A+Y^2B)/\varepsilon^2)$ by a factor of $\widetilde{\mathcal{O}}(\max\{X, Y\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \emph{balanced exploration policies} into their classical counterparts.","Yu Bai is currently a Senior Research Scientist at Salesforce AI Research. Prior to joining Salesforce, Yu completed his PhD in Statistics at Stanford University. Yu’s research interest lies broadly in machine learning, with recent focus on the theoretical foundations of reinforcement learning and games, deep learning, and uncertainty quantification.","yub.jpeg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom2",
"18/04/22","11:00:00","Tianyi Lin","https://tydlin.github.io/","Berkeley","What Can Optimization Theory Offer to Equilibrium Computation?","In this talk, I will first give a brief overview of optimization theory from an algorithmic point of view and position the contribution of our works properly. More specifically, I focus on new problems pertaining to equilibrium computation: nonconvex-concave min-max optimization problems and generalized Nash equilibrium problems. To design the efficient first-order numerical schemes with nonasymptotic theoretical guarantee, we investigate their special structures and give the reasonable optimality condition as well as simple and intuitive numerical schemes. In summary, our works are typical examples in developing new algorithms for equilibrium computation via appeal to basic principles in this history and we hope that our perspective may be useful more broadly.","Tianyi Lin is a PhD student in EECS at UC Berkeley where he is advised by Michael. I. Jordan. Before Berkeley, he received a BA in Mathematics from Nanjing University and a MASt in Mathematics from University of Cambridge. His research interests include algorithmic design and applications to machine learning, especially the multi-agent learning in games and optimal transport.
","tianyilin.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom3","https://arxiv.org/abs/1906.00331;(https://arxiv.org/abs/2204.03132"
12/04/22,11:00:00,"Denizalp (Deni) Goktas","https://www.denizalpgoktas.com/","Brown","Convex-Concave Min-Max Stackelberg Games","Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. 

In this talk, I will present our recent work which introduced two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald’s maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. Additionally, in our paper, we observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game. Further, we demonstrate the efficacy and efficiency of our algorithms in practice by computing competitive equilibria in Fisher markets with varying utility structures. Our experiments suggest potential ways to extend our theoretical results, by demonstrating how different smoothness properties can affect the convergence rate of our algorithms.","Deni is a third-year PhD student in the computer science department at Brown advised by Amy Greenwald. He is interested in decentralized multi-agent learning in markets and games, with the ultimate goal of building decentralized welfare improving technology based on these algorithms. Prior to coming to Brown, he received a bachelor’s degree in computer science-statistics from Columbia, and a bachelor’s degree in political science-economics from the Paris Institute of Political Studies (Sciences Po). He is a recipient of the JP Morgan fellowship.","denizalp.jpg","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom4","https://arxiv.org/abs/2110.05192;https://arxiv.org/abs/2203.14126"
12/04/22,12:00:00,"Juan Carlos Perdomo","https://jcperdomo.org/","Berkeley","Performative Prediction","In this talk, I'll give a self-contained introduction to performative prediction, a recently developed framework for thinking about prediction systems in social settings and the underlying feedback loops they generate. This talk will be focused on the motivations and basic results. As part of the discussion, I hope to highlight connections with other common frameworks (e.g RL, algorithmic game theory) for dealing with predictions in dynamical systems. Time permitting, I'll give an overview of various technical extensions that have been established so far and outline ongoing research directions.","Juan C. Perdomo is a PhD student in EECS at UC Berkeley where he is advised by Moritz Hardt and Peter Bartlett. Before Berkeley, he received a BA in computer science and mathematics from Harvard College. His research focuses on the theoretical foundations of machine learning, with a particular focus on learning in dynamic environments.","perdomo.png","Room 116 [in person]","https://berkeley.zoom.us/my/simonszoom5","https://arxiv.org/pdf/2002.06673;https://arxiv.org/pdf/2006.06887;https://arxiv.org/pdf/2102.08570"
