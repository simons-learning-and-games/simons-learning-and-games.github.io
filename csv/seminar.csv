"date","time","speaker","url","affiliation","title","bio","abstract","imgfile","location","zoom","connected-papers"
"10/02/22","13:30:00","Niao He","https://odi.inf.ethz.ch/niaohe.html","ETH","Universal Acceleration for Minimax Optimization","Niao He is currently an Assistant Professor in the Department of Computer Science at ETH Zurich,  where she leads the Optimization and Decision Intelligence (ODI) Group. She is an ELLIS Scholar and a core faculty member of ETH AI Center. Previously, she was an assistant professor at the University of Illinois at Urbana-Champaign from 2016 to 2020. Before that, she received her Ph.D. degree in Operations Research from Georgia Institute of Technology in 2015. Her research interests lie in the intersection of optimization and machine learning, with a primary focus on minimax optimization and reinforcement learning.","We present a generic acceleration recipe for smooth minimax optimization. By simply combing with existing solvers such as extra-gradient method as the workhorse for subproblems,  one can achieve best-known convergence rates for minimax optimization in various regimes such as the strongly-convex-(strongly)-concave,  nonconvex-(strongly)-concave settings.  Our key idea is largely inspired by the Catalyst framework in [Lin, Mairal, and Harchaoui, 2015] and can be framed as an inexact accelerated proximal point algorithm. The framework can be extended to solving finite-sum minimax problems and special classes of nonconvex-nonconcave minimax problems with best-known rates.","niao-he.jpg","Room 116 ","https://berkeley.zoom.us/my/simonszoom2",
"03/02/22","13:30:00","Jacob Abernethy","https://faculty.cc.gatech.edu/~jabernethy9/","GA Tech","Building Optimization Algorithms by Playing Games ","Jacob Abernethy is an Associate Professor in the College of Computing at Georgia Tech. In October 2011 he finished a PhD in the Division of Computer Science at the University of California at Berkeley, spent two years as a Simons postdoctoral fellow at UPenn, and held a faculty job at the University of Michigan for four years before joining Georgia Tech. Abernethy's primary interest is in Machine Learning, with a particular focus in sequential decision making, online learning, online algorithms and adversarial learning models ","A very popular trick for solving certain types of optimization problems is this: write your objective as the solution of a two-player zero-sum game, endow both players with an appropriate learning algorithm, watch how the opponents compete, and extract an (approximate) solution from the actions/decisions taken by the players throughout the process. This approach is very generic and provides a natural template to produce new and interesting algorithms. I will describe this framework and show how it applies in several scenarios, and describe recent work drawing connections to classical algorithms including Frank-Wolfe, Nesterov Accelerated Gradient Descent, and many others. ","jake.jpeg","Room 116","https://berkeley.zoom.us/my/simonszoom2",
"17/02/22","14:00:00","Simina Branzei","https://simina.info/","Purdue","Multiplayer learning in multi-armed bandits and markets","Simina Branzei is an assistant professor at Purdue University. She completed her Ph.d. at Aarhus University in Denmark and afterwards was a research fellow at Simons and postdoctoral fellow at the Hebrew University of Jerusalem. Her research interests are in algorithmic game theory, learning, and more generally artificial intelligence and theoretical computer science.","Consider a two player multi-armed bandit problem, where one arm has a known success probability, while the other arm does not. In every round, each player pulls an arm, gets the reward from the arm they pulled, and observes the action of the other player but not their reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards. 

We show that two competing players explore less than a single player with an optimal strategy, while cooperating players explore more. Neutral players learn from each other, receiving strictly higher rewards than if they played by themselves. Both competing and neutral players settle on the same arm in the long term.

 

I will also discuss exchange and production markets with additive valuations, when players learn to bid using proportional response dynamics. In the production market, this dynamic leads to growth of the market in the long term, but also creates growing inequality between the players. In the exchange market, the proportional response dynamic converges to market equilibria. This resolves an open question about the exchange market, where tatonnement does not converge to market equilibria and no natural process leading to equilibria was known.

This is based on joint works with Peres and Devanur, Mehta, Nisan, and Rabani.","simina.png","Room 116","https://berkeley.zoom.us/my/simonszoom2",
"03/03/22","14:00:00","Negin Golrezaei","http://www.mit.edu/~golrezae/","MIT","Offline-to-Online Transformation via Blackwell Approachability","Negin Golrezaei is the KDD Career Development Professor in Communications and Technology and an Assistant Professor of Operations Management at the MIT Sloan School of Management. Her current research interests are in the area of machine learning, statistical learning theory, mechanism design, and optimization algorithms with applications to revenue management, pricing, and online markets. Before joining MIT, Negin spent a year as a postdoctoral fellow at Google Research in New York where she worked with the Market Algorithm team to develop, design, and test new mechanisms and algorithms for online marketplaces. She received her Ph.D. (2017) in operations research from USC. Negin is the recipient of several awards including the 2021 Young Investigator Award at ONR, the 2018 Google Faculty Research Award, and the 2017 George B. Dantzig Dissertation Award.","Black-box reduction from offline optimization to online algorithms has been the topic of study of numerous works (Kalai and Vempala (2005); Dudík et al. (2017); Kakade et al. (2009); Hazan and Koren (2016)). In this work, we consider designing online algorithms for problems with greedy approximation algorithms which are robust to local errors. Our first result is a reduction which gives a $O(\sqrt{T})$-regret algorithm for the full information setting via Blackwell Approachability. Next, we introduce Bandit Blackwell Sequential Games which is a bandit version of Blackwell Approachability, and leverage this to get a reduction which gives a $O(T^{2/3})$-regret algorithm in the bandit setting. We further show several applications of these reductions to problems including submodular maximization, reserve pricing in auctions, and assortment optimization. ","negin.png","Room 116","https://berkeley.zoom.us/my/simonszoom2",
"10/03/22","14:00:00","Gidel Gauthier","https://gauthiergidel.github.io/","Université de Montréal","Extragradient Method: O(1/K) Last-Iterate Convergence for Monotone Variational Inequalities and Connections With Cocoercivity","The central part of our analysis is based on Performance Estimation Problems and computer-assistant proofs. In the talk, I will pay special attention to this approach and explain the main non-trivial issues we faced on the way of getting the final proofs via numerical computations.","Extragradient method (EG) is one of the most popular methods for solving saddle point and variational inequalities problems (VIP). Despite its long history and significant attention in the optimization community, there remain important open questions about the convergence of EG. In this paper, we resolve one of such questions and derive the first last-iterate O(1/K) convergence rate for EG for monotone and Lipschitz VIP without any additional assumptions on the operator. The rate is given in terms of reducing the squared norm of the operator. Moreover, we establish several results on the (non-)cocoercivity of the update operators of EG, Optimistic Gradient Method, and Hamiltonian Gradient Method, when the original operator is monotone and Lipschitz.   The central part of our analysis is based on Performance Estimation Problems and computer-assistant proofs. In the talk, I will pay special attention to this approach and explain the main non-trivial issues we faced on the way of getting the final proofs via numerical computations.","gidel.jpg","Room 116","https://berkeley.zoom.us/my/simonszoom2","https://arxiv.org/abs/2110.04261"
"15/03/22","14:00:00","Jeff Shamma","https://ise.illinois.edu/directory/profile/jshamma","Illinois Urbana-Champaign","Higher-order learning in games and feedback control","Jeff S. Shamma is with the University of Illinois at Urbana-Champaign where he is the Department Head of Industrial and Enterprise Systems Engineering (ISE) and Jerry S. Dobrovolny Chair in ISE. His prior academic appointments include faculty positions at the King Abdullah University of Science and Technology (KAUST), where he is an Adjunct Professor of Electrical and Computer Engineering, and the Georgia Institute of Technology, where he was the Julian T. Hightower Chair in Systems and Controls. Jeff received a PhD in Systems Science and Engineering from MIT in 1988. He is a Fellow of IEEE and IFAC; a recipient of the IFAC High Impact Paper Award, AACC Donald P. Eckman Award, and NSF Young Investigator Award; and a past Distinguished Lecturer of the IEEE Control Systems Society. He has been a plenary or semi-plenary speaker at several conferences, including NeurIPS, World Congress of the Game Theory Society, IEEE Conference on Decision and Control, and the American Control Conference. Jeff is currently serving as the Editor-in-Chief for the IEEE Transactions on Control of Network Systems.","In game theoretic learning, e.g., for matrix games and population games, agents myopically adapt their strategies in reaction to the evolving strategies of other agents in an effort to maximize their own utilities. The resulting interactions can be represented as a dynamical system that maps agent observations to agent strategies. Well-known and widely studied examples of adaptation/learning rules include fictitious play, gradient play, regret minimization, and replicator dynamics.  In these examples, the associated learning rule has an induced dimensionality, or number of states, that is equal to the number of agent actions. As the terminology suggests, “higher-order” learning refers to learning rules that are not restricted in their dimensionality. Such learning rules introduce auxiliary states not included in their lower-order counterparts, while respecting the original informational structure of what is observed and known to each agent. 

 

This talk presents on overview of results that illustrate how higher-order learning can induce qualitative changes in long-run outcomes, including convergence to Nash equilibria not possible under lower-order dynamics (including uncoupled dynamics counterexamples and replicator dynamics for zero-sum games). A specific focus will be on higher-order “anticipatory” versions of lower-order learning rules, which appears to parallel optimistic versions of optimization algorithms. The talk concludes with an analysis framework for higher-order learning that exploits an implicit feedback structure in game-theoretic learning, where the learning dynamics are separated from the game specifics. In particular, the talk presents the concept of passivity from feedback control, its application to higher-order learning analysis, and connections to contractive/stable games.","shamma.png","Room 116","https://berkeley.zoom.us/my/simonszoom2",
24/03/22,14:00:00,"Vidya Muthukumar","https://vmuthukumar.ece.gatech.edu/","GA Tech","A fundmental tension between no-regret and last-iterate convergence under realization-based feedback","Vidya Muthukumar is an Assistant Professor in the Schools of Electrical and Computer Engineering and Industrial and Systems Engineering at Georgia Institute of Technology. Her broad interests are in game theory, online and statistical learning. She is particularly interested in designing learning algorithms that provably adapt in strategic environments, the intersection of algorithmic game theory and reinforcement learning, and the theory of overparameterized machine learning.

 

Vidya received the PhD degree in Electrical Engineering and Computer Sciences from University of California, Berkeley. She is the recipient of the Adobe Data Science Research Award, the Simons-Berkeley Research Fellowship (for the Fall 2020 program on ""Theory of Reinforcement Learning""), and a Georgia Tech Class of 1969 Teaching Fellowship for the academic year 2021-2022. 

","We study the limiting behavior of the mixed strategies that result from optimal no-regret learning strategies in a repeated game setting where the stage game is a 2 × 2 zero-sum game or competitive game (i.e. the Nash equilibrium is unique, strictly mixed, and trivially computable). We consider optimal no-regret algorithms that are approximately mean-based and monotonic in their argument. We show that for any such algorithm, the limiting mixed strategies of the players cannot converge almost surely to any Nash or correlated equilibrium. This negative result is also shown to hold under a broad relaxation of these assumptions, including popular variants of Online-Mirror-Descent with optimism or data-adaptive step-sizes, and parameter-free online algorithms. Our results identify the inherent stochasticity in players’ realizations as a critical factor underlying this divergence, and demonstrate a crucial difference in outcomes between using the opponent’s mixtures and realizations to make updates. 

 

We conclude with a brief description of some open problems in last-iterate and time-averaged convergence under realization-based feedback.

 

This talk reflects joint work with Soham Phade (Salesforce) and Anant Sahai (UC Berkeley), and is based on the paper: https://arxiv.org/pdf/2012.02125.pdf 

","vidya.jpg","Room 116","https://berkeley.zoom.us/my/simonszoom2",
