"date","time","speaker","url","affiliation","title","bio","abstract","imgfile","location","zoom","connected-papers"
"10/02/22","13:30:00","Niao He","https://odi.inf.ethz.ch/niaohe.html","ETH","Universal Acceleration for Minimax Optimization","Niao He is currently an Assistant Professor in the Department of Computer Science at ETH Zurich,  where she leads the Optimization and Decision Intelligence (ODI) Group. She is an ELLIS Scholar and a core faculty member of ETH AI Center. Previously, she was an assistant professor at the University of Illinois at Urbana-Champaign from 2016 to 2020. Before that, she received her Ph.D. degree in Operations Research from Georgia Institute of Technology in 2015. Her research interests lie in the intersection of optimization and machine learning, with a primary focus on minimax optimization and reinforcement learning.","We present a generic acceleration recipe for smooth minimax optimization. By simply combing with existing solvers such as extra-gradient method as the workhorse for subproblems,  one can achieve best-known convergence rates for minimax optimization in various regimes such as the strongly-convex-(strongly)-concave,  nonconvex-(strongly)-concave settings.  Our key idea is largely inspired by the Catalyst framework in [Lin, Mairal, and Harchaoui, 2015] and can be framed as an inexact accelerated proximal point algorithm. The framework can be extended to solving finite-sum minimax problems and special classes of nonconvex-nonconcave minimax problems with best-known rates.","niao-he.jpg","Room 116 ","https://berkeley.zoom.us/my/simonszoom2",
"03/02/22","13:30:00","Jacob Abernethy","https://faculty.cc.gatech.edu/~jabernethy9/","GA Tech","Building Optimization Algorithms by Playing Games ","Jacob Abernethy is an Associate Professor in the College of Computing at Georgia Tech. In October 2011 he finished a PhD in the Division of Computer Science at the University of California at Berkeley, spent two years as a Simons postdoctoral fellow at UPenn, and held a faculty job at the University of Michigan for four years before joining Georgia Tech. Abernethy's primary interest is in Machine Learning, with a particular focus in sequential decision making, online learning, online algorithms and adversarial learning models ","A very popular trick for solving certain types of optimization problems is this: write your objective as the solution of a two-player zero-sum game, endow both players with an appropriate learning algorithm, watch how the opponents compete, and extract an (approximate) solution from the actions/decisions taken by the players throughout the process. This approach is very generic and provides a natural template to produce new and interesting algorithms. I will describe this framework and show how it applies in several scenarios, and describe recent work drawing connections to classical algorithms including Frank-Wolfe, Nesterov Accelerated Gradient Descent, and many others. ","jake.jpeg","Room 116","https://berkeley.zoom.us/my/simonszoom2",
17/03/22,15:30:00,"Angela Zhou","https://angelamzhou.github.io/","Simons","Minimax-Optimal Policy Learning and Evaluation under Unobserved Confounding
","Angela is a Research Fellow at Simons Program on Causality. In Summer 2022 she will start as an Assistant Professor at University of Southern California Marshall School of Business in Data Sciences and Operations. Her research interests are in statistical machine learning for data-driven (sequential) decision making under uncertainty, causal inference, and the interplay of statistics and optimization. She is particularly interested in applications-motivated methodology with guarantees in order to bridge method and practice.","We study the problem of learning (single-timestep) personalized decision policies from observational data while accounting for possible unobserved confounding. This problem is called ""batch learning with bandit feedback"" in computer science or ""optimal decision/treatment rules"" in biostatistics, ""optimal policy learning"" in econometrics. Previous approaches, which assume unconfoundedness, that is, that no unobserved confounders affect both the treatment assignment as well as outcome, can lead to policies that introduce harm rather than benefit when some unobserved confounding is present as is generally the case with observational data. Instead, because policy value and regret may not be point-identifiable, we study a robust method that minimizes the worst-case estimated regret of a candidate policy against a baseline policy over an uncertainty set for propensity weights that controls the extent of unobserved confounding. Our uncertainty sets are superpopulation versions of sensitivity analysis in causal inference. We prove generalization guarantees that ensure our policy is safe when applied in practice and in fact obtains the best possible uniform control on the range of all possible population regrets that agree with the possible extent of confounding. We develop efficient algorithmic solutions to compute this minimax-optimal policy. Finally, we assess and compare our methods on synthetic and semisynthetic data; including a case study on personalizing hormone replacement therapy based on observational data in which we illustrate our results on a randomized experiment.","angelazhou.jpg","Room 116","https://berkeley.zoom.us/my/simonszoom2","https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2020.3699"
