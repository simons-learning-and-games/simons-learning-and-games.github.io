<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <link href="schedule.css" rel="stylesheet">
    <script src="script.js" type="text/javascript"></script>
    <base target="_top">
  </head>
  <body>
    <script src="script.js" type="text/javascript"></script>
    <ul class="timeline list">
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 1</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 03 February, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116):
            <a href="https://faculty.cc.gatech.edu/~jabernethy9/">Jacob Abernethy</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>A very popular trick for solving certain types of optimization problems is this: write your objective as the solution of a two-player zero-sum game, endow both players with an appropriate learning algorithm, watch how the opponents compete, and extract an (approximate) solution from the actions/decisions taken by the players throughout the process. This approach is very generic and provides a natural template to produce new and interesting algorithms. I will describe this framework and show how it applies in several scenarios, and describe recent work drawing connections to classical algorithms including Frank-Wolfe, Nesterov Accelerated Gradient Descent, and many others. </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Jacob Abernethy is an Associate Professor in the College of Computing at Georgia Tech. In October 2011 he finished a PhD in the Division of Computer Science at the University of California at Berkeley, spent two years as a Simons postdoctoral fellow at UPenn, and held a faculty job at the University of Michigan for four years before joining Georgia Tech. Abernethy's primary interest is in Machine Learning, with a particular focus in sequential decision making, online learning, online algorithms and adversarial learning models </p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 2</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 10 February, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 ):
            <a href="https://odi.inf.ethz.ch/niaohe.html">Niao He</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>We present a generic acceleration recipe for smooth minimax optimization. By simply combing with existing solvers such as extra-gradient method as the workhorse for subproblems,  one can achieve best-known convergence rates for minimax optimization in various regimes such as the strongly-convex-(strongly)-concave,  nonconvex-(strongly)-concave settings.  Our key idea is largely inspired by the Catalyst framework in [Lin, Mairal, and Harchaoui, 2015] and can be framed as an inexact accelerated proximal point algorithm. The framework can be extended to solving finite-sum minimax problems and special classes of nonconvex-nonconcave minimax problems with best-known rates.</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Niao He is currently an Assistant Professor in the Department of Computer Science at ETH Zurich,  where she leads the Optimization and Decision Intelligence (ODI) Group. She is an ELLIS Scholar and a core faculty member of ETH AI Center. Previously, she was an assistant professor at the University of Illinois at Urbana-Champaign from 2016 to 2020. Before that, she received her Ph.D. degree in Operations Research from Georgia Institute of Technology in 2015. Her research interests lie in the intersection of optimization and machine learning, with a primary focus on minimax optimization and reinforcement learning.</p>
            </div>
          </li>
        </ul>
      </li>
    </ul>
    <script src="collapsible.js"></script>
  </body>
</html>
