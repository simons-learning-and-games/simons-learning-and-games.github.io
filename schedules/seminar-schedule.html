<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <link href="schedule.css" rel="stylesheet">
    <script src="script.js" type="text/javascript"></script>
    <base target="_top">
  </head>
  <body>
    <script src="script.js" type="text/javascript"></script>
    <ul class="timeline list">
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 1</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 03 February, 2022 13:30</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Talk (Room 116):
            <a href="https://faculty.cc.gatech.edu/~jabernethy9/">Jacob Abernethy</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>A very popular trick for solving certain types of optimization problems is this: write your objective as the solution of a two-player zero-sum game, endow both players with an appropriate learning algorithm, watch how the opponents compete, and extract an (approximate) solution from the actions/decisions taken by the players throughout the process. This approach is very generic and provides a natural template to produce new and interesting algorithms. I will describe this framework and show how it applies in several scenarios, and describe recent work drawing connections to classical algorithms including Frank-Wolfe, Nesterov Accelerated Gradient Descent, and many others. </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Jacob Abernethy is an Associate Professor in the College of Computing at Georgia Tech. In October 2011 he finished a PhD in the Division of Computer Science at the University of California at Berkeley, spent two years as a Simons postdoctoral fellow at UPenn, and held a faculty job at the University of Michigan for four years before joining Georgia Tech. Abernethy's primary interest is in Machine Learning, with a particular focus in sequential decision making, online learning, online algorithms and adversarial learning models </p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 2</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 10 February, 2022 13:30</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Talk (Room 116 ):
            <a href="https://odi.inf.ethz.ch/niaohe.html">Niao He</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>We present a generic acceleration recipe for smooth minimax optimization. By simply combing with existing solvers such as extra-gradient method as the workhorse for subproblems,  one can achieve best-known convergence rates for minimax optimization in various regimes such as the strongly-convex-(strongly)-concave,  nonconvex-(strongly)-concave settings.  Our key idea is largely inspired by the Catalyst framework in [Lin, Mairal, and Harchaoui, 2015] and can be framed as an inexact accelerated proximal point algorithm. The framework can be extended to solving finite-sum minimax problems and special classes of nonconvex-nonconcave minimax problems with best-known rates.</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Niao He is currently an Assistant Professor in the Department of Computer Science at ETH Zurich,  where she leads the Optimization and Decision Intelligence (ODI) Group. She is an ELLIS Scholar and a core faculty member of ETH AI Center. Previously, she was an assistant professor at the University of Illinois at Urbana-Champaign from 2016 to 2020. Before that, she received her Ph.D. degree in Operations Research from Georgia Institute of Technology in 2015. Her research interests lie in the intersection of optimization and machine learning, with a primary focus on minimax optimization and reinforcement learning.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 3</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 17 February, 2022 14:00</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Talk (Room 116):
            <a href="https://simina.info/">Simina Branzei</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>Consider a two player multi-armed bandit problem, where one arm has a known success probability, while the other arm does not. In every round, each player pulls an arm, gets the reward from the arm they pulled, and observes the action of the other player but not their reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards. 

We show that two competing players explore less than a single player with an optimal strategy, while cooperating players explore more. Neutral players learn from each other, receiving strictly higher rewards than if they played by themselves. Both competing and neutral players settle on the same arm in the long term.

 

I will also discuss exchange and production markets with additive valuations, when players learn to bid using proportional response dynamics. In the production market, this dynamic leads to growth of the market in the long term, but also creates growing inequality between the players. In the exchange market, the proportional response dynamic converges to market equilibria. This resolves an open question about the exchange market, where tatonnement does not converge to market equilibria and no natural process leading to equilibria was known.

This is based on joint works with Peres and Devanur, Mehta, Nisan, and Rabani.</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Simina Branzei is an assistant professor at Purdue University. She completed her Ph.d. at Aarhus University in Denmark and afterwards was a research fellow at Simons and postdoctoral fellow at the Hebrew University of Jerusalem. Her research interests are in algorithmic game theory, learning, and more generally artificial intelligence and theoretical computer science.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 4</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 03 March, 2022 14:00</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Talk (Room 116):
            <a href="http://www.mit.edu/~golrezae/">Negin Golrezaei</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>Black-box reduction from offline optimization to online algorithms has been the topic of study of numerous works (Kalai and Vempala (2005); Dudík et al. (2017); Kakade et al. (2009); Hazan and Koren (2016)). In this work, we consider designing online algorithms for problems with greedy approximation algorithms which are robust to local errors. Our first result is a reduction which gives a $O(\sqrt{T})$-regret algorithm for the full information setting via Blackwell Approachability. Next, we introduce Bandit Blackwell Sequential Games which is a bandit version of Blackwell Approachability, and leverage this to get a reduction which gives a $O(T^{2/3})$-regret algorithm in the bandit setting. We further show several applications of these reductions to problems including submodular maximization, reserve pricing in auctions, and assortment optimization. </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Negin Golrezaei is the KDD Career Development Professor in Communications and Technology and an Assistant Professor of Operations Management at the MIT Sloan School of Management. Her current research interests are in the area of machine learning, statistical learning theory, mechanism design, and optimization algorithms with applications to revenue management, pricing, and online markets. Before joining MIT, Negin spent a year as a postdoctoral fellow at Google Research in New York where she worked with the Market Algorithm team to develop, design, and test new mechanisms and algorithms for online marketplaces. She received her Ph.D. (2017) in operations research from USC. Negin is the recipient of several awards including the 2021 Young Investigator Award at ONR, the 2018 Google Faculty Research Award, and the 2017 George B. Dantzig Dissertation Award.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 5</span>
        </span>
        <p>
          <span class="bf">
            <span>Thursday 10 March, 2022 14:00</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Talk (Room 116):
            <a href="https://gauthiergidel.github.io/">Gidel Gauthier</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>Extragradient method (EG) is one of the most popular methods for solving saddle point and variational inequalities problems (VIP). Despite its long history and significant attention in the optimization community, there remain important open questions about the convergence of EG. In this paper, we resolve one of such questions and derive the first last-iterate O(1/K) convergence rate for EG for monotone and Lipschitz VIP without any additional assumptions on the operator. The rate is given in terms of reducing the squared norm of the operator. Moreover, we establish several results on the (non-)cocoercivity of the update operators of EG, Optimistic Gradient Method, and Hamiltonian Gradient Method, when the original operator is monotone and Lipschitz.   The central part of our analysis is based on Performance Estimation Problems and computer-assistant proofs. In the talk, I will pay special attention to this approach and explain the main non-trivial issues we faced on the way of getting the final proofs via numerical computations.<br><br>Papers connected with this talk:
                <ol>
                  <li>https://arxiv.org/abs/2110.04261</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>The central part of our analysis is based on Performance Estimation Problems and computer-assistant proofs. In the talk, I will pay special attention to this approach and explain the main non-trivial issues we faced on the way of getting the final proofs via numerical computations.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 6</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 15 March, 2022 14:00</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Talk (Room 116):
            <a href="https://ise.illinois.edu/directory/profile/jshamma">Jeff Shamma</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>In game theoretic learning, e.g., for matrix games and population games, agents myopically adapt their strategies in reaction to the evolving strategies of other agents in an effort to maximize their own utilities. The resulting interactions can be represented as a dynamical system that maps agent observations to agent strategies. Well-known and widely studied examples of adaptation/learning rules include fictitious play, gradient play, regret minimization, and replicator dynamics.  In these examples, the associated learning rule has an induced dimensionality, or number of states, that is equal to the number of agent actions. As the terminology suggests, “higher-order” learning refers to learning rules that are not restricted in their dimensionality. Such learning rules introduce auxiliary states not included in their lower-order counterparts, while respecting the original informational structure of what is observed and known to each agent. 

 

This talk presents on overview of results that illustrate how higher-order learning can induce qualitative changes in long-run outcomes, including convergence to Nash equilibria not possible under lower-order dynamics (including uncoupled dynamics counterexamples and replicator dynamics for zero-sum games). A specific focus will be on higher-order “anticipatory” versions of lower-order learning rules, which appears to parallel optimistic versions of optimization algorithms. The talk concludes with an analysis framework for higher-order learning that exploits an implicit feedback structure in game-theoretic learning, where the learning dynamics are separated from the game specifics. In particular, the talk presents the concept of passivity from feedback control, its application to higher-order learning analysis, and connections to contractive/stable games.</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Jeff S. Shamma is with the University of Illinois at Urbana-Champaign where he is the Department Head of Industrial and Enterprise Systems Engineering (ISE) and Jerry S. Dobrovolny Chair in ISE. His prior academic appointments include faculty positions at the King Abdullah University of Science and Technology (KAUST), where he is an Adjunct Professor of Electrical and Computer Engineering, and the Georgia Institute of Technology, where he was the Julian T. Hightower Chair in Systems and Controls. Jeff received a PhD in Systems Science and Engineering from MIT in 1988. He is a Fellow of IEEE and IFAC; a recipient of the IFAC High Impact Paper Award, AACC Donald P. Eckman Award, and NSF Young Investigator Award; and a past Distinguished Lecturer of the IEEE Control Systems Society. He has been a plenary or semi-plenary speaker at several conferences, including NeurIPS, World Congress of the Game Theory Society, IEEE Conference on Decision and Control, and the American Control Conference. Jeff is currently serving as the Editor-in-Chief for the IEEE Transactions on Control of Network Systems.</p>
            </div>
          </li>
        </ul>
      </li>
    </ul>
    <script src="collapsible.js"></script>
  </body>
</html>
