<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <link href="schedule.css" rel="stylesheet">
    <script src="script.js" type="text/javascript"></script>
    <base target="_top">
  </head>
  <body>
    <script src="script.js" type="text/javascript"></script>
    <ul class="timeline list">
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 1</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 08 February, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116):
            <a href="http://mlanctot.info/">Marc Lanctot</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. In this talk I will cover AlphaGo, an approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0, and later to defeat a previous Go professional Lee Sedol. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go. Toward the end of the talk, I will give a short survey of what has happened since this landmark event in AI and some open challenges for continued work in this area.<br><br>Papers connected with this talk:
                <ol>
                  <li>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Marc Lanctot is a research scientist at DeepMind. Previously, he was a post-doctoral researcher at the Maastricht University Games and AI Group, working with Mark Winands. During his PhD, he worked at University of Alberta with Michael Bowling on sampling algorithms for equilibrium computation and decision-making in games. Before his PhD, he did an undergrad and Master's at McGill University's School of Computer Science and Games Research @ McGill, under the supervision of Clark Verbrugge. He is interested in general multiagent learning (and planning), computational game theory, reinforcement learning, and game-tree search.</p>
            </div>
          </li>
        </ul>
        <ul style="list-style-type:disc;">
          <li>Session 2 (Room 116 ):
            <a href="https://dtsipras.com/">Dimitris Tsipras</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>While modern machine learning systems can succeed on challenging benchmarks, they are quite brittle: their performance significantly degrades when exposed to even small variations of their training environments.
How can we build ML models that are more robust? In this talk, I will present a framework for reasoning about the worst-case behavior of our machine learning systems. I will then describe how this framework can be used in practice to train models that are invariant to a broad family of worst-case input perturbations. Finally, I will discuss how such robust learning can be fundamentally different from standard learning, presenting a unique set of challenges and opportunities.<br><br>Papers connected with this talk:
                <ol>
                  <li>Towards Deep Learning Models Resistant to Adversarial Attacks</li>
                  <li>Robustness May Be at Odds with Accuracy</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Dimitris Tsipras is a postdoctoral researcher at Stanford, advised by Percy Liang and Greg Valiant. He obtained his PhD from MIT, where he was advised by Aleksander Madry. His work ocuses on understanding and improving the reliability of modern machine learning methods.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 2</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 15 February, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 (in person)):
            <a href="http://www.cs.columbia.edu/~binghuip/">Binghui Peng</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.<br><br>Papers connected with this talk:
                <ol>
                  <li>Efficient Regret Minimization in Non-Convex Games</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Binghui Peng is a third-year PhD student (Fall 2019 - present) in the computer science department of Columbia University, advised by Prof. Christos Papadimitriou and Prof. Xi Chen. He is broadly interested in theoretical computer science, especially its intersection with machine learning and game theory. In summer 2020, he worked in Google research NYC, hosted by Sara Ahmadian. Previously, he got his bachelor degree from Yao class, Tsinghua Univeristy, where he worked with Prof. Pingzhong Tang on game theory.</p>
            </div>
          </li>
        </ul>
        <ul style="list-style-type:disc;">
          <li>Session 2 (Zoom):
            <a href="https://kiranvodrahalli.github.io/about/">Kiran Vodrahalli</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>On-line firms deploy suites of software platforms, where each platform is designed to interact with users during a certain activity, such as browsing, chatting, socializing, emailing, driving, etc. The economic and incentive structure of this exchange, as well as its algorithmic nature, have not been explored to our knowledge. We model this interaction as a Stackelberg game between a Designer and one or more Agents. We model an Agent as a Markov chain whose states are activities; we assume that the Agent's utility is a linear function of the steady-state distribution of this chain. The Designer may design a platform for each of these activities/states; if a platform is adopted by the Agent, the transition probabilities of the Markov chain are affected, and so is the objective of the Agent. The Designer's utility is a linear function of the steady state probabilities of the accessible states minus the development cost of the platforms. The underlying optimization problem of the Agent -- how to choose the states for which to adopt the platform -- is an MDP. If this MDP has a simple yet plausible structure (the transition probabilities from one state to another only depend on the target state and the recurrent probability of the current state) the Agent's problem can be solved by a greedy algorithm. The Designer's optimization problem (designing a custom suite for the Agent so as to optimize, through the Agent's optimum reaction, the Designer's revenue), is NP-hard to approximate within any finite ratio; however, the special case, while still NP-hard, has an FPTAS. These results generalize from a single Agent to a distribution of Agents with finite support, as well as to the setting where the Designer must find the best response to the existing strategies of other Designers. We discuss other implications of our results and directions of future research. </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Kiran Vodrahalli is a 5th year Ph.D. student at Columbia University advised by Daniel Hsu and Alex Andoni. He previously received a B.A. in Mathematics and an M.S.E in Computer Science from Princeton University, where he was advised by Sanjeev Arora.</p>
            </div>
          </li>
        </ul>
      </li>
    </ul>
    <script src="collapsible.js"></script>
  </body>
</html>
