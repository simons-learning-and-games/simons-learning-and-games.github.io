<!DOCTYPE html>
<html>
  <head>
    <title></title>
    <link href="schedule.css" rel="stylesheet">
    <script src="script.js" type="text/javascript"></script>
    <base target="_top">
  </head>
  <body>
    <script src="script.js" type="text/javascript"></script>
    <ul class="timeline list">
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 1</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 08 February, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 [in person]):
            <a href="http://mlanctot.info/">Marc Lanctot</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. In this talk I will cover AlphaGo, an approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0, and later to defeat a previous Go professional Lee Sedol. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go. Toward the end of the talk, I will give a short survey of what has happened since this landmark event in AI and some open challenges for continued work in this area.<br><br>Papers connected with this talk:
                <ol>
                  <li>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play.</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Marc Lanctot is a research scientist at DeepMind. Previously, he was a post-doctoral researcher at the Maastricht University Games and AI Group, working with Mark Winands. During his PhD, he worked at University of Alberta with Michael Bowling on sampling algorithms for equilibrium computation and decision-making in games. Before his PhD, he did an undergrad and Master's at McGill University's School of Computer Science and Games Research @ McGill, under the supervision of Clark Verbrugge. He is interested in general multiagent learning (and planning), computational game theory, reinforcement learning, and game-tree search.</p>
            </div>
          </li>
        </ul>
        <ul style="list-style-type:disc;">
          <li>Session 2 (Room 116 [in person]):
            <a href="https://dtsipras.com/">Dimitris Tsipras</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>While modern machine learning systems can succeed on challenging benchmarks, they are quite brittle: their performance significantly degrades when exposed to even small variations of their training environments.
How can we build ML models that are more robust? In this talk, I will present a framework for reasoning about the worst-case behavior of our machine learning systems. I will then describe how this framework can be used in practice to train models that are invariant to a broad family of worst-case input perturbations. Finally, I will discuss how such robust learning can be fundamentally different from standard learning, presenting a unique set of challenges and opportunities.<br><br>Papers connected with this talk:
                <ol>
                  <li>Towards Deep Learning Models Resistant to Adversarial Attacks</li>
                  <li>Robustness May Be at Odds with Accuracy</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Dimitris Tsipras is a postdoctoral researcher at Stanford, advised by Percy Liang and Greg Valiant. He obtained his PhD from MIT, where he was advised by Aleksander Madry. His work ocuses on understanding and improving the reliability of modern machine learning methods.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 2</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 15 February, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 [in person]):
            <a href="http://www.cs.columbia.edu/~binghuip/">Binghui Peng</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.<br><br>Papers connected with this talk:
                <ol>
                  <li>Efficient Regret Minimization in Non-Convex Games</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Binghui Peng is a third-year PhD student (Fall 2019 - present) in the computer science department of Columbia University, advised by Prof. Christos Papadimitriou and Prof. Xi Chen. He is broadly interested in theoretical computer science, especially its intersection with machine learning and game theory. In summer 2020, he worked in Google research NYC, hosted by Sara Ahmadian. Previously, he got his bachelor degree from Yao class, Tsinghua Univeristy, where he worked with Prof. Pingzhong Tang on game theory.</p>
            </div>
          </li>
        </ul>
        <ul style="list-style-type:disc;">
          <li>Session 2 (Zoom):
            <a href="https://kiranvodrahalli.github.io/about/">Kiran Vodrahalli</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>On-line firms deploy suites of software platforms, where each platform is designed to interact with users during a certain activity, such as browsing, chatting, socializing, emailing, driving, etc. The economic and incentive structure of this exchange, as well as its algorithmic nature, have not been explored to our knowledge. We model this interaction as a Stackelberg game between a Designer and one or more Agents. We model an Agent as a Markov chain whose states are activities; we assume that the Agent's utility is a linear function of the steady-state distribution of this chain. The Designer may design a platform for each of these activities/states; if a platform is adopted by the Agent, the transition probabilities of the Markov chain are affected, and so is the objective of the Agent. The Designer's utility is a linear function of the steady state probabilities of the accessible states minus the development cost of the platforms. The underlying optimization problem of the Agent -- how to choose the states for which to adopt the platform -- is an MDP. If this MDP has a simple yet plausible structure (the transition probabilities from one state to another only depend on the target state and the recurrent probability of the current state) the Agent's problem can be solved by a greedy algorithm. The Designer's optimization problem (designing a custom suite for the Agent so as to optimize, through the Agent's optimum reaction, the Designer's revenue), is NP-hard to approximate within any finite ratio; however, the special case, while still NP-hard, has an FPTAS. These results generalize from a single Agent to a distribution of Agents with finite support, as well as to the setting where the Designer must find the best response to the existing strategies of other Designers. We discuss other implications of our results and directions of future research. </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Kiran Vodrahalli is a 5th year Ph.D. student at Columbia University advised by Daniel Hsu and Alex Andoni. He previously received a B.A. in Mathematics and an M.S.E in Computer Science from Princeton University, where he was advised by Sanjeev Arora.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 3</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 01 March, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 [in person]):
            <a href="https://chavdarova.github.io/">Tatjana Chavdarova</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>There will be two parts, in the first one I will present the work by Hsieh, Mertikopoulos and Cevher from ICML 2021, and in the second part I will present our joint work with Manolis Zampetakis and Michael I. Jordan -- see their abstracts below, respectively.

Compared to minimization, the min-max optimization in machine learning applications is considerably more convoluted because of the existence of cycles and similar phenomena. Such oscillatory behaviors are well-understood in the convex-concave regime, and many algorithms are known to overcome them. In this paper, we go beyond this basic setting and characterize the convergence properties of many popular methods in solving non-convex/non-concave problems. In particular, we show that a wide class of state-of-the-art schemes and heuristics may converge with arbitrarily high probability to attractors that are in no way min-max optimal or even stationary. Our work thus points out a potential pitfall among many existing theoretical frameworks, and we corroborate our theoretical claims by explicitly showcasing spurious attractors in simple two-dimensional problems.

Several widely-used first-order saddle point optimization methods yield an identical continuous-time ordinary differential equation (ODE) to that of the Gradient Descent Ascent (GDA) method when derived naively. However, their convergence properties are very different even on simple bilinear games. We use a technique from fluid dynamics called High-Resolution Differential Equations (HRDEs) to design ODEs of several saddle point optimization methods. For bilinear games, the convergence properties of the derived HRDEs correspond to that of the starting discrete methods. Using these techniques, we show that the HRDE of Optimistic Gradient Descent Ascent (OGDA) has last-iterate convergence for general monotone variational inequalities. To our knowledge, this is the first proof in continuous-time for monotone variational inequalities. Moreover, we provide the rates for the best-iterate convergence of the OGDA method, relying solely on the first-order smoothness of the monotone operator.

<br><br>Papers connected with this talk:
                <ol>
                  <li>https://arxiv.org/pdf/2112.13826.pdf</li>
                  <li>https://arxiv.org/pdf/2112.13826.pdf


</li>
                </ol>
              </p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Tatjana Chavdarova is a Postdoctoral Researcher at the Department of Electrical Engineering and Computer Science (EECS) at the University of California at Berkeley, working with Michael Jordan. Prior to that she was postdoc at the Machine Learning and Optimization (MLO) lab at EPFL, working with Martin Jaggi, and she obtained her Ph.D. from EPFL, and Idiap, supervised by François Fleuret. She is primarily interested in the intersection of game theory and machine learning, but also generalization, robustness, and generative modeling.</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 4</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 08 March, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 [in person]):
            <a href="https://www.linkedin.com/in/jasonmili">Jason Milionis</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>A basic question that has arisen ever since the genesis of the concept of the Nash Equilibrium [Nash, 1951] is whether it can adequately capture the long-term behavior of players who play a game repeatedly. Considering the case where the players’ behavior is represented by a rule/map from the current mixed strategy to the next, this becomes a problem in the realm of dynamical systems. In this context, we will ponder to what extent we might (or might not) expect Nash Equilibria to be attractors of dynamical systems. At the same time, we will wonder: What does it even mean for a dynamical system to converge, when one might observe chaotic-like behavior? Our journey will include an expansive introduction to some notions of the mathematical field of Algebraic Topology, and we will showcase the power of topological arguments in our effort to understand the limits of Nash Equilibria. The culmination of that discussion will be a general impossibility result: there exist games for which no game dynamics’ long-term behavior can be described by Nash Equilibria in a satisfactory way. A similar, but even stronger, result holds, if we had desired that approximate Nash Equilibria be the entire &quot;set of fate&quot; of any game dynamics; there, there is a positive-measure set of games for which the set of approximate Nash Equilibria cannot accurately characterize the fate of the dynamical system. Nonetheless, complexity will also manifest itself in our discussion, since we will show that for non-degenerate games in particular, we can construct such game dynamics, albeit with complexity-wise obstacles this time, and we will end our exploration with our conjecture that constructing such a game dynamics for non-degenerate games is PPAD-complete.

The talk is based on a recently submitted joint work with Christos Papadimitriou (Columbia University), Georgios Piliouras (Singapore University of Technology and Design), and Kelly Spendlove (University of Oxford).</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Jason Milionis is a first-year Ph.D. student in the Computer Science Department at Columbia University, advised by Christos Papadimitriou and Tim Roughgarden. He is broadly interested in Game Theory, especially in combination with Machine Learning. Previously, he graduated with a bachelor's degree in Electrical and Computer Engineering with a major in Computer Science from the National Technical University of Athens (NTUA).


</p>
            </div>
          </li>
        </ul>
      </li>
      <li>
        <span class="itemlabel">
          <span class="hbox llap">Day 5</span>
        </span>
        <p>
          <span class="bf">
            <span>Tuesday 15 March, 2022</span>
          </span>
        </p>
        <ul style="list-style-type:disc;">
          <li>Session 1 (Room 116 [in person]):
            <a href="https://noahgol.github.io/">Noah Golowich</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>Much of modern learning theory has been split between two regimes: the classical offline setting, where data arrive independently, and the online setting, where data arrive adversarially. While the former model is often both computationally and statistically tractable, the latter re- quires no distributional assumptions. In an attempt to achieve the best of both worlds, previous work proposed the smooth online setting where each sample is drawn from an adversarially chosen distribution, which is smooth, i.e., it has a bounded density with respect to a fixed dom- inating measure. Existing results for the smooth setting were known only for binary-valued function classes and were computationally expensive in general; in this paper, we fill these lacunae. In particular, we provide tight bounds on the minimax regret of learning a nonpara- metric function class, with nearly optimal dependence on both the horizon and smoothness parameters. Furthermore, we provide the first oracle-efficient, no-regret algorithms in this set- ting. In particular, we propose an oracle-efficient improper algorithm whose regret achieves optimal dependence on the horizon and a proper algorithm requiring only a single oracle call per round whose regret has the optimal horizon dependence in the classification setting and is sublinear in general. Both algorithms have exponentially worse dependence on the smoothness parameter of the adversary than the minimax rate. We then prove a lower bound on the oracle complexity of any proper learning algorithm, which matches the oracle-efficient upper bounds up to a polynomial factor, thus demonstrating the existence of a statistical-computational gap in smooth online learning. Finally, we apply our results to the contextual bandit setting to show that if a function class is learnable in the classical setting, then there is an oracle-efficient, no-regret algorithm for contextual bandits in the case that contexts arrive in a smooth manner.</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Noah Golowich is a PhD Student at Massachusetts Institute of Technology, advised by Constantinos Daskalakis. His research interests are in theoretical machine learning, with particular focus on the connections between multi-agent learning, game theory, and online learning. He is a recipient of a Fannie &amp; John Hertz Foundation Fellowship and an NSF Graduate Fellowship.</p>
            </div>
          </li>
        </ul>
        <ul style="list-style-type:disc;">
          <li>Session 2 (Room 116 [in person]):
            <a href="https://bahh723.github.io/">Chen-yu Wei</a>
            <button class="collapsible collapsible-abstract"></button>
            <div class="content">
              <p>Multi-agent reinforcement learning (MARL) problems are challenging due to information asymmetry. To overcome this challenge, existing methods often require high level of coordination or communication between the agents. We consider two-agent multi-armed bandits (MABs) and Markov decision processes (MDPs) with a hierarchical information structure arising in applications, which we exploit to propose simpler and more efficient algorithms that require no coordination or communication. In the structure, in each step the “leader” chooses her action first, and then the “follower” decides his action after observing the leader’s action. The two agents observe the same reward (and the same state transition in the MDP setting) that depends on their joint action. For the bandit setting, we propose a hierarchical bandit algorithm that achieves a near-optimal gap-independent regret of Oe( √ ABT) and a near-optimal gap-dependent regret of O(log(T )), where A and B are the numbers of actions of the leader and the follower, respectively, and T is the number of steps. We further extend to the case of multiple followers and the case with a deep hierarchy, where we both obtain near-optimal regret bounds. For the MDP setting, we obtain Oe( √ H7S2ABT ) regret, where H is the number of steps per episode, S is the number of states, T is the number of episodes. This matches the existing lower bound in terms of A, B, and T .</p>
            </div>
            <button class="collapsible collapsible-bio"></button>
            <div class="content">
              <p>Chen-Yu Wei is a fifth-year Computer Science PhD student at University of Southern California. He received M.S. in Communication Engineering and B.S. in Electrical Engineering from National Taiwan University. His research focuses on designing provable algorithms for online decision making, reinforcement learning, and learning in games. He is a recipient of the Best Paper Award at COLT 2021, and the Simons-Berkeley Research Fellowship. </p>
            </div>
          </li>
        </ul>
      </li>
    </ul>
    <script src="collapsible.js"></script>
  </body>
</html>
